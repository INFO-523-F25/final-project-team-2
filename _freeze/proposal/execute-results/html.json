{
  "hash": "cd36c764dc7956d92f399432b22d180f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Music Genre Classification & Audio Feature Analysis\"\nsubtitle: \"Proposal\"\nauthor: \n  - name: \"Team 2 - David Kennedy, Elsie Bold, Kim Rosema\"\n    affiliations:\n      - name: \"College of Information Science, University of Arizona\"\ndescription: \"Project description\"\nformat:\n  html:\n    code-tools: true\n    code-overflow: wrap\n    code-line-numbers: true\n    embed-resources: true\neditor: visual\ncode-annotations: hover\nexecute:\n  warning: false\njupyter: python3\n---\n\n::: {#load-pkgs .cell message='false' execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n```\n:::\n\n\n## Dataset\n\n\nA brief description of your dataset including its provenance, dimensions, etc. as well as the reason why you chose this dataset.\n\nDataset Description\nFor this project, we will use a subset of the Million Song Dataset (MSD), a large-scale collection of audio features and metadata for one million contemporary popular music tracks. The data originates from The Echo Nest, a music intelligence platform that analyzed songs and extracted detailed features such as tempo, loudness, danceability, timbre, and pitch.\nAfter loading a sample of the dataset into Python, here is an example structure:\n\n::: {#e69924d0 .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\ndata = pd.read_csv('data/msd_subset_features.csv')\ndata.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n(10000, 19)\n```\n:::\n:::\n\n\nThis subset contains 10,000 songs and 19 attributes covering metadata (artist, title, year) and numerical features (tempo, energy, loudness, etc.). A preview of key variables includes:\n\n- song_id: Unique identifier for each song (Echo Nest ID)\n- artist_name: Name of the artist\n- title: Song title\n- tempo: Estimated tempo in beats per minute (BPM)\n- danceability:  Measure of how suitable a track is for dancing (0.0 to 1.0)\n- energy: Overall energy of the song (0.0 to 1.0)\n- loudness: Overall loudness in decibels (dB)\n- year: Year of release (0 indicates unknown)\n- song_hotttnesss: Song popularity measure (0.0 to 1.0, may contain null values)\n- We chose this dataset because it combines rich audio signal features with real-world metadata, making it ideal for exploring both classification (e.g., genre prediction) and regression (e.g., popularity prediction). It also offers opportunities to learn about feature extraction from raw audio and deep learning techniques like CNNs for signal processing.\n- Make sure to load the data and use inline code for some of this information.\n\n## Questions\n\n1. Can we accurately classify a song’s genre using its extracted audio features?\n2. What audio and metadata features are most predictive of a song’s popularity (song_hotttnesss)?\n\n## Analysis plan\n\n- Question 1 — Genre Classification\nGoal: Predict the genre label (obtained from artist_terms or merged from FMA dataset).\nVariables Used:\ndanceability, energy, tempo, loudness, key, mode, time_signature, duration, year.\nFeature Engineering:\nAggregate timbre and pitch arrays (mean, std).\nEncode categorical features (mode, key).\nModeling Approach:\nApply dimensionality reduction (PCA).\nTrain classification models: Random Forest, Gradient Boosting, and Neural Network.\nEvaluate accuracy, precision, recall, and F1-score.\nVisualization:\nFeature importance plot, confusion matrix, PCA scatter of top features.\n- Question 2 — Popularity Prediction\nGoal: Model song_hotttnesss as a continuous variable.\nVariables Used:\nSame audio features as above, plus artist_terms_weight, year, duration.\nFeature Engineering:\nCompute mean and variance of segment-level features.\nNormalize continuous variables.\nModeling Approach:\nLinear Regression, Random Forest Regressor, and XGBoost.\nEvaluate with RMSE and R² metrics.\nVisualization:\nCorrelation heatmap between features and popularity.\nPartial dependence plots to interpret influential factors.\nExternal Data\nOptionally, merge Spotify API audio features to cross-check and expand the dataset, especially for missing or inconsistent fields (danceability, valence, energy).\n\nBy combining metadata and extracted audio features, this project will demonstrate how machine learning can reveal musical patterns and predict real-world outcomes such as genre and popularity.\n\n",
    "supporting": [
      "proposal_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}