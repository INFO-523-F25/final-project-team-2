---
title: "Music Genre Classification & Audio Feature Analysis"
subtitle: "Proposal"
author: 
  - name: "Team 2 - David Kennedy, Elsie Bold, Kim Rosema"
    affiliations:
      - name: "College of Information Science, University of Arizona"
description: "Project description"
format:
  html:
    code-tools: true
    code-overflow: wrap
    code-line-numbers: true
    embed-resources: true
editor: visual
code-annotations: hover
execute:
  warning: false
jupyter: python3
---

```{python}
#| label: load-pkgs
#| message: false
import numpy as np
```

## Dataset

```{python}
#| label: load-dataset
#| message: false
```

A brief description of your dataset including its provenance, dimensions, etc. as well as the reason why you chose this dataset.

Dataset Description
For this project, we will use a subset of the Million Song Dataset (MSD), a large-scale collection of audio features and metadata for one million contemporary popular music tracks. The data originates from The Echo Nest, a music intelligence platform that analyzed songs and extracted detailed features such as tempo, loudness, danceability, timbre, and pitch.
After loading a sample of the dataset into Python, here is an example structure:
```{python}
import pandas as pd
data = pd.read_hdf('msd_sample.h5', key='metadata')
data.shape
```

This subset contains 10,000 songs and 54 attributes covering metadata (artist, title, year) and numerical features (tempo, energy, loudness, etc.). A preview of key variables includes:
artist_name: name of the artist
title: track title
tempo: estimated beats per minute
danceability: measure of rhythm suitability for dancing (0–1)
energy: measure of intensity and activity (0–1)
loudness: overall loudness in decibels
segments_timbre: MFCC-like timbral coefficients
segments_pitches: chroma features (harmonic structure)
year: release year
song_hotttnesss: popularity score (0–1)
We chose this dataset because it combines rich audio signal features with real-world metadata, making it ideal for exploring both classification (e.g., genre prediction) and regression (e.g., popularity prediction). It also offers opportunities to learn about feature extraction from raw audio and deep learning techniques like CNNs for signal processing.
Make sure to load the data and use inline code for some of this information.

## Questions

Can we accurately classify a song’s genre using its extracted audio features?
What audio and metadata features are most predictive of a song’s popularity (song_hotttnesss)?

## Analysis plan

- Question 1 — Genre Classification
Goal: Predict the genre label (obtained from artist_terms or merged from FMA dataset).
Variables Used:
danceability, energy, tempo, loudness, key, mode, time_signature, segments_timbre_mean, segments_pitches_mean, duration, year.
Feature Engineering:
Aggregate timbre and pitch arrays (mean, std).
Encode categorical features (mode, key).
Modeling Approach:
Apply dimensionality reduction (PCA).
Train classification models: Random Forest, Gradient Boosting, and Neural Network.
Evaluate accuracy, precision, recall, and F1-score.
Visualization:
Feature importance plot, confusion matrix, PCA scatter of top features.
- Question 2 — Popularity Prediction
Goal: Model song_hotttnesss as a continuous variable.
Variables Used:
Same audio features as above, plus artist_terms_weight, year, duration.
Feature Engineering:
Compute mean and variance of segment-level features.
Normalize continuous variables.
Modeling Approach:
Linear Regression, Random Forest Regressor, and XGBoost.
Evaluate with RMSE and R² metrics.
Visualization:
Correlation heatmap between features and popularity.
Partial dependence plots to interpret influential factors.
External Data
Optionally, merge Spotify API audio features to cross-check and expand the dataset, especially for missing or inconsistent fields (danceability, valence, energy).

By combining metadata and extracted audio features, this project will demonstrate how machine learning can reveal musical patterns and predict real-world outcomes such as genre and popularity.

