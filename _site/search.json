[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Music Genre Classification & Audio Feature Analysis",
    "section": "",
    "text": "import numpy as np"
  },
  {
    "objectID": "proposal.html#dataset",
    "href": "proposal.html#dataset",
    "title": "Music Genre Classification & Audio Feature Analysis",
    "section": "Dataset",
    "text": "Dataset\nA brief description of your dataset including its provenance, dimensions, etc. as well as the reason why you chose this dataset.\nDataset Description For this project, we will use a subset of the Million Song Dataset (MSD), a large-scale collection of audio features and metadata for one million contemporary popular music tracks. The data originates from The Echo Nest, a music intelligence platform that analyzed songs and extracted detailed features such as tempo, loudness, danceability, timbre, and pitch. After loading a sample of the dataset into Python, here is an example structure:\n\nimport pandas as pd\ndata = pd.read_csv('data/msd_subset_features.csv')\ndata.shape\n\n(10000, 19)\n\n\nThis subset contains 10,000 songs and 19 attributes covering metadata (artist, title, year) and numerical features (tempo, energy, loudness, etc.). A preview of key variables includes:\n\nsong_id: Unique identifier for each song (Echo Nest ID)\nartist_name: Name of the artist\ntitle: Song title\ntempo: Estimated tempo in beats per minute (BPM)\ndanceability: Measure of how suitable a track is for dancing (0.0 to 1.0)\nenergy: Overall energy of the song (0.0 to 1.0)\nloudness: Overall loudness in decibels (dB)\nyear: Year of release (0 indicates unknown)\nsong_hotttnesss: Song popularity measure (0.0 to 1.0, may contain null values)\nWe chose this dataset because it combines rich audio signal features with real-world metadata, making it ideal for exploring both classification (e.g., genre prediction) and regression (e.g., popularity prediction). It also offers opportunities to learn about feature extraction from raw audio and deep learning techniques like CNNs for signal processing.\nMake sure to load the data and use inline code for some of this information."
  },
  {
    "objectID": "proposal.html#questions",
    "href": "proposal.html#questions",
    "title": "Music Genre Classification & Audio Feature Analysis",
    "section": "Questions",
    "text": "Questions\n\nCan we accurately classify a song’s genre using its extracted audio features?\nWhat audio and metadata features are most predictive of a song’s popularity (song_hotttnesss)?"
  },
  {
    "objectID": "proposal.html#analysis-plan",
    "href": "proposal.html#analysis-plan",
    "title": "Music Genre Classification & Audio Feature Analysis",
    "section": "Analysis plan",
    "text": "Analysis plan\n\nQuestion 1 — Genre Classification Goal: Predict the genre label (obtained from artist_terms or merged from FMA dataset). Variables Used: danceability, energy, tempo, loudness, key, mode, time_signature, duration, year. Feature Engineering: Aggregate timbre and pitch arrays (mean, std). Encode categorical features (mode, key). Modeling Approach: Apply dimensionality reduction (PCA). Train classification models: Random Forest, Gradient Boosting, and Neural Network. Evaluate accuracy, precision, recall, and F1-score. Visualization: Feature importance plot, confusion matrix, PCA scatter of top features.\nQuestion 2 — Popularity Prediction Goal: Model song_hotttnesss as a continuous variable. Variables Used: Same audio features as above, plus artist_terms_weight, year, duration. Feature Engineering: Compute mean and variance of segment-level features. Normalize continuous variables. Modeling Approach: Linear Regression, Random Forest Regressor, and XGBoost. Evaluate with RMSE and R² metrics. Visualization: Correlation heatmap between features and popularity. Partial dependence plots to interpret influential factors. External Data Optionally, merge Spotify API audio features to cross-check and expand the dataset, especially for missing or inconsistent fields (danceability, valence, energy).\n\nBy combining metadata and extracted audio features, this project will demonstrate how machine learning can reveal musical patterns and predict real-world outcomes such as genre and popularity."
  }
]